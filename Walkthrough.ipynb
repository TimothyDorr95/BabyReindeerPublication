{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook walks you through the steps used for the Baby Reindeer paper. \n",
    "# Table of Content\n",
    "1. Preprocessing and Cleaning of Data\n",
    "2. Data Enrichment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from detoxify import Detoxify\n",
    "import random\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing and Cleaning of Data\n",
    "In this section, we used the text column to create variables such as user name, episode, etc. We also add an ID varaible and create a variable that links a comment to it's parent comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read main df that has everything\n",
    "df_all = pd.read_csv('data/bbreindeer.csv')\n",
    "print(df_all.shape)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from text, we should be able to extract the comment number (first number surrounded by white space), the user name (first string of characters and numbers, surrounded by /t again), and the comment itself (the rest of the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in text, get the first element as string that is contained by \\t\n",
    "def get_first_element(text):\n",
    "    return text.split('\\t')[1]\n",
    "\n",
    "#in text, get the second element as string that is contained by \\t\n",
    "def get_second_element(text):\n",
    "    return text.split('\\t')[2]\n",
    "\n",
    "#in text, get the third element as string that is contained by \\t\n",
    "def get_third_element(text):\n",
    "    return text.split('\\t')[3]\n",
    "\n",
    "df_all['Comment_level'] = df_all['text'].apply(get_first_element)\n",
    "df_all['User_name'] = df_all['text'].apply(get_second_element)\n",
    "df_all['Comment'] = df_all['text'].apply(get_third_element)\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets add an id variable as well as a parent id variable, in case a comment is a response to a different comment. In such cases we would like to be able to link the comment back to the parent comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add an id number to it, starting from 100\n",
    "df_all['id'] = range(100, 100+len(df_all))\n",
    "#add an indicator variable to which comment the current comment is replying to\n",
    "df_all['parent_id'] = None\n",
    "\n",
    "# Iterate through the DataFrame to assign parent_ids\n",
    "for idx in df_all.index:\n",
    "    current_level = df_all.at[idx, 'Comment_level']\n",
    "    # Scan previous rows to find the parent comment\n",
    "    for j in range(idx - 1, -1, -1):\n",
    "        if df_all.at[j, 'Comment_level'] < current_level:\n",
    "            df_all.at[idx, 'parent_id'] = df_all.at[j, 'id']\n",
    "            break\n",
    "        \n",
    "#save as clean master file\n",
    "df_all.to_csv('data/bbreindeer_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Enrichment\n",
    "There are various columns we need to create to make the data more useful.\n",
    "* Is Martha/Fiona Mentioned\n",
    "* Is Donny mentioned\n",
    "* General sentiment of the comment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Is Martha/Fiona Mentioned?\n",
    "This will be a simple binary column, 1 if Martha or Fiona is mentioned, 0 if not. The mention can either be explicit (using her name) or implicit (using pronouns like she, her, or name calling. etc.) The explicit mentions are pretty easy to do, the implicit mentions will be harder, as we will have to use GPT to figure out who the comment is talking about.\n",
    "\n",
    "We will \n",
    "* First look for explicit mentions of Martha\n",
    "* Then look for implcit mentions of a female\n",
    "* then use GPT to classify the comments that are not explicitly mentioning martha, but are implicitly mentioning a female --> the response should be is this about martha, terry, other, or unsure. \n",
    "* then we can collaps the variables to a simple binary martha variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_martha = df_all\n",
    "martha_words = ['martha', 'fiona']\n",
    "df_martha['martha_explicit'] = df_martha['Comment'].apply(lambda x: any(word in x.lower() for word in martha_words))\n",
    "df_martha['martha_explicit'] = df_martha['martha_explicit'].astype(int)\n",
    "df_martha.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check how often she is mentioned explicitly\n",
    "df_martha['martha_explicit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She is being mentioned explicitly 133 times, lets see how often she is being mentioned indirectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirect_words = ['her', 'she', 'stalker']\n",
    "pattern = r'\\b(?:' + '|'.join(indirect_words) + r')\\b'\n",
    "\n",
    "df_martha['indirect_female'] = df_martha['Comment'].apply(lambda x: bool(re.search(pattern, x.lower())))\n",
    "df_martha['indirect_female'] = df_martha['indirect_female'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check how often we have indirect mentions, without having martha present\n",
    "print(df_martha[(df_martha['indirect_female'] == 1) & (df_martha['martha_explicit'] == 0)].shape[0])\n",
    "print(df_martha[(df_martha['indirect_female'] == 1)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 235 indirect female mentions, in 171 of these, Martha is not mentioned directly as well, so we gotta figure out whether those are talking about martha or not. For this we will have GPT assist us with the classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "Your task is to evaluate whether a reddit comment is about the main character, Martha, or not. Martha is an overweight stalker of Donny. Martha also claims to be a lawyer. \n",
    "Another character who this comment might be refering to is Teri, who is transexual, and a therapist by profession. Your job is to figure out whether the comment referes to Martha, or whether it refers to someone else (such as Teri). \n",
    "The comment in question is: ###{}####. \"\"\"\n",
    "\n",
    "prompt_2 = \"\"\"This comment is a reply to the following comment: ###{}###.\n",
    "\"\"\" \n",
    "\n",
    "prompt_3 = \"\"\"Respond with 1 if this comment is about martha, respond 2 if it is not, respond 3 if you are unsure. Only respond with the number. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_creator(row):\n",
    "    comment = row['Comment']\n",
    "    #check the comment_level\n",
    "    if row['Comment_level'] == 0:\n",
    "        prompt = prompt_1.format(comment) + prompt_3\n",
    "        return prompt\n",
    "    else:\n",
    "        parent_id = row['parent_id']\n",
    "        parent_comment = df_martha[df_martha['id'] == parent_id]['Comment'].values[0]\n",
    "        prompt = prompt_1.format(comment) + prompt_2.format(parent_comment) + prompt_3\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load api key and initialize client\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY_ME\")\n",
    "\n",
    "client = OpenAI(api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_GPT(prompt, model = 'gpt-4o', temperature = 0):\n",
    "    \"\"\"\n",
    "    Runs the specified GPT model with the given prompt and temperature.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt for the GPT model.\n",
    "        model (str, optional): The model to use. Defaults to 'gpt-4o'.\n",
    "        temperature (float, optional): The temperature parameter for the GPT model. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        str: The response text from the GPT model.\n",
    "    \"\"\"\n",
    "  #other models are gpt-4-turbo, gpt-4, gpt-3.5-turbo-16k\n",
    "\n",
    "\n",
    "  #get the response\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \n",
    "               \"content\": prompt}],\n",
    "    temperature = temperature\n",
    "  )\n",
    "\n",
    "  #get the response\n",
    "    response_text = response.choices[0].message.content\n",
    "    return response_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_martha['GPT_result'] = None\n",
    "for index, row in df_martha.iterrows():\n",
    "    #check if the comment needs classification\n",
    "    needs_classification = False\n",
    "    if row['martha_explicit'] == 0 and row['indirect_female'] == 1:\n",
    "        needs_classification = True\n",
    "\n",
    "    if needs_classification:\n",
    "        prompt = prompt_creator(row)\n",
    "        result = run_GPT(prompt)\n",
    "        #make sure that it is only a number\n",
    "        result = result.replace('\\n', '')\n",
    "        result = result.replace(' ', '')\n",
    "        #remove all letters\n",
    "        result = ''.join([i for i in result if i.isdigit()])\n",
    "        #add result to the GPT result column\n",
    "        df_martha.at[index, 'GPT_result'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cehck the df where GPT_result is not None\n",
    "df_martha[df_martha['GPT_result'].notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save csv\n",
    "df_martha.to_csv('data/bbreindeer_clean_with_GPT_for_Martha.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will have to \n",
    "* a) evaluate how useful the responses are\n",
    "* b) code the unsure manually\n",
    "* c) use these new variables together with the old ones to create the variable indicating whether martha is mentioned or not. \n",
    "\n",
    "##### Evaluation of GPT\n",
    "Here, I will code a random subsample of the data to see how well GPT is doing. I will first create a random subsample of the data, then I will create a functon that lets me code it, and finally I will evaluate the results (only for the martha or not martha). n(1 martha, 2 not martha, 3 unsure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_martha = pd.read_csv('data/bbreindeer_clean_with_GPT_for_Martha.csv')\n",
    "random.seed(123)\n",
    "df_sub = df_martha[df_martha['GPT_result'].notnull()]\n",
    "df_sub = df_sub[df_sub['GPT_result']<3]\n",
    "df_sub = df_sub.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['Human_result'] = None\n",
    "i = 0\n",
    "for index, row in df_sub.iterrows():\n",
    "    #check if the comment needs classification\n",
    "\n",
    "    prompt = prompt_creator(row)\n",
    "    #get user coding\n",
    "    coding = input(prompt)\n",
    "    df_sub.at[index, 'Human_result'] = coding\n",
    "    i += 1\n",
    "    print(i/len(df_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "df_sub.to_csv('data/martha_presence_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets manually evaluate the instances GPT was unsure about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code the unsure comments\n",
    "df_unsure = df_martha[df_martha['GPT_result']==3]\n",
    "df_unsure['human_lable'] = None\n",
    "i = 0\n",
    "\n",
    "for index, row in df_unsure.iterrows():\n",
    "    #check if the comment needs classification\n",
    "    prompt = prompt_creator(row)\n",
    "    #get user coding\n",
    "    coding = input(prompt)\n",
    "    df_unsure.at[index, 'GPT_result'] = coding\n",
    "    i +=1\n",
    "    print(i/len(df_unsure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine back\n",
    "df_unsure = df_unsure[['id', 'GPT_result']]\n",
    "df_martha = df_martha[['id', 'GPT_result']]\n",
    "#delete observations in df_martha that are in df_unsure\n",
    "df_martha = df_martha[~df_martha['id'].isin(df_unsure['id'])]\n",
    "\n",
    "#concatenate the two dfs\n",
    "df_martha = pd.concat([df_martha, df_unsure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the lables\n",
    "df_martha.to_csv('data/martha_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Is Donny/Richard Mentioned?\n",
    "Same as above but for Donny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donny_words = ['donny', 'richard']\n",
    "df_donny = df_martha\n",
    "df_donny['donny_explicit'] = df_donny['Comment'].apply(lambda x: any(word in x.lower() for word in donny_words))\n",
    "df_donny['donny_explicit'] = df_donny['donny_explicit'].astype(int)\n",
    "df_donny.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirect_words = ['he', 'his', 'him', 'dude']\n",
    "pattern = r'\\b(?:' + '|'.join(indirect_words) + r')\\b'\n",
    "\n",
    "df_donny['indirect_male'] = df_donny['Comment'].apply(lambda x: bool(re.search(pattern, x.lower())))\n",
    "df_donny['indirect_male'] = df_donny['indirect_male'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how often donny was mentioned explicitly\n",
    "print(df_donny['donny_explicit'].value_counts())\n",
    "#check how often there is an implicit mention\n",
    "print(df_donny['indirect_male'].value_counts())\n",
    "#check how often there is an implicit mention without an explicit mention\n",
    "print(df_donny[(df_donny['indirect_male'] == 1) & (df_donny['donny_explicit'] == 0)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He is mentioned explicitly 140 times, there are 426 indirect mentions, and out of those, in 333 cases, Donny is not mentioned explicitly. Those are the ones wee need to classify with GPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "Your task is to evaluate whether a reddit comment is about the main character, Donny, or not. Donny is a bartender and standup comedian who is being stalked by a women named Martha, and who for a while dates a transexual called Teri. The comment in question is: ###{}####. \"\"\"\n",
    "\n",
    "prompt_2 = \"\"\"This comment is a reply to the following comment: ###{}###.\n",
    "\"\"\" \n",
    "\n",
    "prompt_3 = \"\"\"Respond with 1 if this comment is about Donny, respond 2 if it is not, respond 3 if you are unsure. Only respond with the number. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_donny['GPT_result'] = None\n",
    "for index, row in df_donny.iterrows():\n",
    "    #check if the comment needs classification\n",
    "    needs_classification = False\n",
    "    if row['donny_explicit'] == 0 and row['indirect_male'] == 1:\n",
    "        needs_classification = True\n",
    "\n",
    "    if needs_classification:\n",
    "        prompt = prompt_creator(row)\n",
    "        result = run_GPT(prompt)\n",
    "        #make sure that it is only a number\n",
    "        result = result.replace('\\n', '')\n",
    "        result = result.replace(' ', '')\n",
    "        #remove all letters\n",
    "        result = ''.join([i for i in result if i.isdigit()])\n",
    "        #add result to the GPT result column\n",
    "        df_donny.at[index, 'GPT_result'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_donny.to_csv('data/bbreindeer_clean_with_GPT_for_Donny.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation of GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_donny = pd.read_csv(\"data/bbreindeer_clean_with_GPT_for_Donny.csv\")\n",
    "random.seed(123)\n",
    "df_sub = df_donny[df_donny['GPT_result'].notnull()]\n",
    "df_sub = df_sub[df_sub['GPT_result']<3]\n",
    "df_sub = df_sub.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['Human_result'] = None\n",
    "i = 0\n",
    "for index, row in df_sub.iterrows():\n",
    "    #check if the comment needs classification\n",
    "\n",
    "    prompt = prompt_creator(row)\n",
    "    #get user coding\n",
    "    coding = input(prompt)\n",
    "    df_sub.at[index, 'Human_result'] = coding\n",
    "    i += 1\n",
    "    print(i/len(df_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('data/Donny_presence_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code the unsure comments\n",
    "df_unsure = df_donny[df_donny['GPT_result']==3]\n",
    "df_unsure['human_lable'] = None\n",
    "i = 0\n",
    "\n",
    "for index, row in df_unsure.iterrows():\n",
    "    #check if the comment needs classification\n",
    "    prompt = prompt_creator(row)\n",
    "    #get user coding\n",
    "    coding = input(prompt)\n",
    "    df_unsure.at[index, 'GPT_result'] = coding\n",
    "    i +=1\n",
    "    print(i/len(df_unsure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unsure = df_unsure[['id', 'GPT_result']]\n",
    "df_donny = df_donny[['id', 'GPT_result']]\n",
    "#delete observations in df_martha that are in df_unsure\n",
    "df_donny = df_donny[~df_donny['id'].isin(df_unsure['id'])]\n",
    "\n",
    "#concatenate the two dfs\n",
    "df_donny = pd.concat([df_donny, df_unsure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_donny.to_csv('data/donny_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Add Vader Sentiment\n",
    "Lets get some general sentiment scores for all the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vader = pd.read_csv(\"data/bbreindeer_clean.csv\")\n",
    "#create object for sentiment analysis\n",
    "vader_sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new columns for sentiment analysis\n",
    "\n",
    "df_vader['Vader_pos'] = df_vader['Comment'].apply(lambda x: vader_sentiment.polarity_scores(x)['pos'])\n",
    "df_vader['Vader_neg'] = df_vader['Comment'].apply(lambda x: vader_sentiment.polarity_scores(x)['neg'])\n",
    "df_vader['Vader_neu'] = df_vader['Comment'].apply(lambda x: vader_sentiment.polarity_scores(x)['neu'])\n",
    "df_vader['Vader_compound'] = df_vader['Comment'].apply(lambda x: vader_sentiment.polarity_scores(x)['compound'])\n",
    "\n",
    "df_vader.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "df_vader.to_csv('data/bbreindeer_vader.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Combine Data to Master Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add indicator variables and other ones for the master df\n",
    "df_all = pd.read_csv('data/bbreindeer_clean.csv')\n",
    "#df_martha\n",
    "df_martha = pd.read_csv('data/martha_labels.csv')\n",
    "#df_donny\n",
    "df_donny = pd.read_csv('data/donny_labels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_martha.rename(columns = {'GPT_result': 'martha'}, inplace = True)\n",
    "#remove NaN values from martha variable\n",
    "df_martha = df_martha[df_martha['martha'].notnull()]\n",
    "df_martha['martha'] = df_martha['martha'].astype(int)\n",
    "\n",
    "df_martha = df_martha[['id', 'martha']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_donny.rename(columns = {'GPT_result': 'donny'}, inplace = True)\n",
    "#remove NaN values from donny variable\n",
    "df_donny = df_donny[df_donny['donny'].notnull()]\n",
    "df_donny['donny'] = df_donny['donny'].astype(int)\n",
    "\n",
    "df_donny = df_donny[['id', 'donny']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.merge(df_martha, on = 'id', how = 'left')\n",
    "df_all = df_all.merge(df_donny, on = 'id', how = 'left')\n",
    "df_all['martha'].fillna(2, inplace = True)\n",
    "df_all['donny'].fillna(2, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vader = pd.read_csv('data/bbreindeer_vader.csv')\n",
    "df_vader = df_vader[['id', 'Vader_pos', 'Vader_neg', 'Vader_neu', 'Vader_compound']]\n",
    "df_all = df_all.merge(df_vader, on = 'id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn 2 in martha and donny into 0\n",
    "df_all['martha'] = df_all['martha'].replace(2, 0)\n",
    "df_all['donny'] = df_all['donny'].replace(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv again\n",
    "df_all.to_csv('data/MasterData.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Create some sub dataframes\n",
    "Here we just create some dataframes that only have specific people mentioned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_martha_only = df_all[(df_all['martha'] == 1) & (df_all['donny'] == 0)]\n",
    "df_donny_only = df_all[(df_all['martha'] == 0) & (df_all['donny'] == 1)]\n",
    "df_donny_and_martha = df_all[(df_all['martha'] == 1) & (df_all['donny'] == 1)]\n",
    "#random subsample to code\n",
    "random.seed(123)\n",
    "df_to_code = df_martha_only.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_martha_only.to_csv('data/martha_only_all.csv', index = False)\n",
    "df_donny_only.to_csv('data/donny_only_all.csv', index = False)\n",
    "df_donny_and_martha.to_csv('data/donny_and_martha_all.csv', index = False)\n",
    "df_to_code.to_csv('data/to_code_sim_aug_11.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sencond Data Enrichment Stage\n",
    "Now, we are looking for the presence of specific themes in the comments about specific people. Specifically, is the comment empathetic towards the person, is Donny being seen as responsible, and is martha seen as a typical stalker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_d_1 = \"\"\"\n",
    "###Instructions###\n",
    "You are an assistant for a content analysis project, and your job is to classify whether specific themes are present in a given reddit comment. The comments are about the main character Donny in the TV show Baby Reindeer, and the themes you should code for are ‘Empathy’, and ‘Responsibility . It is possible that none, one, or both of these themes are present in every given topics. Here a description of the themes.\n",
    "\n",
    "###Themes \n",
    "Empathy: Assess whether the comment reflects an understanding or supportive acknowledgment of Donny’s feelings and experiences. Empathy is indicated by expressions that recognize Donny's perspective or emotional state in a way that shows sympathy or compassion. Code this theme as 1 if empathy is present, 0 if it is not present, and as 99 if it is unclear. \n",
    "\n",
    "Responsibility: Assess whether the comment holds Donny responsible for being stalked. This can be done by illustrating how his decisions facilitate the stalking, how he is leading Martha on, or other types of victim blaming. Code this theme as 1 if the comment talks about Donny being stalked and responsible, code it as 0 if, while acknowledging he is getting stalked, the comment doesn’t blame Donny, and as 99 if the comment does not talk about Donny being stalked at all. \n",
    "\n",
    "For each of those topics, assess whether they are present in the comment below: \n",
    "###Comment###\n",
    "\"\"\"\n",
    "\n",
    "prompt_d_2 = \"\"\"\n",
    "\n",
    "###Output instructions###\n",
    "Return your response in a json with the following format: \n",
    "{\n",
    "Empathy: [response number], \n",
    "Responsibility: [response number],\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(prompt_d_1 + \"test123\" + prompt_d_2)\n",
    "\n",
    "prompt_m_1 = \"\"\"\n",
    " ###Instructions###\n",
    "You are an assistant for a content analysis project, and your job is to classify whether specific themes are present in a given reddit comment. The comments are about the main character Martha in the TV show Baby Reindeer, and the themes you should code for are ‘Empathy’, and ‘Traditional_Stalker. It is possible that none, one, or both of these themes are present in every given topic. Here a description of the themes.\n",
    "\n",
    "###Themes\n",
    "Empathy: Assess whether the comment reflects an understanding or supportive acknowledgment of Martha’s feelings and experiences. Empathy is indicated by expressions that recognize Martha’s perspective or emotional state in a way that shows sympathy or compassion. Code this theme as 1 if empathy is present, 0 if it is not present, and as 99 if it is unclear.\n",
    "\n",
    "Traditional_Stalker: Assess to what extent Martha is being characterised as a stalker in the traditional sense. This includes behaviours such as unwanted contact, including phone calls, texts, and contact via social media, unwanted gifts, showing up/approaching an individual or their family/friends, monitoring, surveillance, property damage, and threats. Additionally, it is often assumed that traditional stalkers have underlying mental health issues.\n",
    "Code a this theme as 1 if Martha is characterised as a traditional stalker in a comment, 0 if the comment talks about her being a stalker but not in the traditional sense, and 99 if the comment is not about her stalking behaviour at all. \n",
    "\n",
    "\n",
    "\n",
    "For each of those topics, assess whether they are active in the comment below: \n",
    "###Comment###\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_m_2 = \"\"\"\n",
    "###Output instructions###\n",
    "Return your response in a json with the following format: \n",
    "{\n",
    "Empathy: [response number], \n",
    "Traditional_Stalker: [response number]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY_ME\")\n",
    "\n",
    "client = OpenAI(api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_GPT(prompt, model = 'gpt-4o', temperature = 0):\n",
    "    \"\"\"\n",
    "    Runs the specified GPT model with the given prompt and temperature.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt for the GPT model.\n",
    "        model (str, optional): The model to use. Defaults to 'gpt-4o'.\n",
    "        temperature (float, optional): The temperature parameter for the GPT model. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        str: The response text from the GPT model.\n",
    "    \"\"\"\n",
    "  #other models are gpt-4-turbo, gpt-4, gpt-3.5-turbo-16k\n",
    "\n",
    "\n",
    "  #get the response\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \n",
    "               \"content\": prompt}],\n",
    "    temperature = temperature\n",
    "  )\n",
    "\n",
    "  #get the response\n",
    "    response_text = response.choices[0].message.content\n",
    "    return response_text  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Donny\n",
    "Classify whether Donny is being seen as a legitimate victim, and whether he is shown empathy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_donny = df[df['donny'] == 1]\n",
    "df_donny['GPT_result'] = None\n",
    "df_donny['Empathy'] = None\n",
    "df_donny['Responsibility'] = None\n",
    "manual_cleaning = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for index, row in df_donny.iterrows():\n",
    "    comment = row['Comment']\n",
    "    prompt = prompt_d_1 + comment + prompt_d_2\n",
    "    #print(prompt)\n",
    "    result = run_GPT(prompt)\n",
    "    #add gpt result\n",
    "    df_donny.at[index, 'GPT_result'] = result\n",
    "    #try cleaning it already\n",
    "    try: \n",
    "        #turn gpt result into json\n",
    "        result = json.loads(result)\n",
    "        #extract value of key \"Empathy\"\n",
    "        empathy = result['Empathy']\n",
    "        #get value of key \"Responsibility\"\n",
    "        responsibility = result['Responsibility']\n",
    "        #add values to df\n",
    "        df_donny.at[index, 'Empathy'] = empathy\n",
    "        df_donny.at[index, 'Responsibility'] = responsibility\n",
    "    except: \n",
    "        manual_cleaning.append(index)\n",
    "        print('Cannot clean')\n",
    "        print(result)\n",
    "\n",
    "    i += 1\n",
    "    print(i/len(df_donny))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the output\n",
    "for index,row in df_donny.iterrows():\n",
    "    to_clean = row['GPT_result']\n",
    "    #split by { and take second, then split by } and take first\n",
    "    to_clean = to_clean.split('{')[1].split('}')[0]\n",
    "    #turn to json\n",
    "    to_clean = json.loads('{' + to_clean + '}')\n",
    "    #print(to_clean)\n",
    "    #add to df\n",
    "    df_donny.at[index, 'Empathy'] = to_clean['Empathy']\n",
    "    df_donny.at[index, 'Responsibility'] = to_clean['Responsibility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df to csv\n",
    "df_donny.to_csv('data/donny_empathy_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Martha\n",
    "Classify whether Martha is being seen as a understandbale stalker, and whether she is shown empathy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_martha = df[df['martha'] == 1]\n",
    "df_martha['GPT_result'] = None\n",
    "df_martha['Empathy'] = None\n",
    "df_martha['Traditional_Stalker'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use gpt to classify marthas comments\n",
    "i = 0\n",
    "for index, row in df_martha.iterrows():\n",
    "    comment = row['Comment']\n",
    "    prompt = prompt_m_1 + comment + prompt_m_2\n",
    "    #print(prompt)\n",
    "    result = run_GPT(prompt)\n",
    "    df_martha.at[index, 'GPT_result'] = result\n",
    "\n",
    "    i += 1\n",
    "    print(i/len(df_martha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean output\n",
    "fix_manually = []\n",
    "\n",
    "for index,row in df_martha.iterrows():\n",
    "    to_clean = row['GPT_result']\n",
    "    #split by { and take second, then split by } and take first\n",
    "    to_clean = to_clean.split('{')[1].split('}')[0]\n",
    "    #turn to json\n",
    "    #print(to_clean)\n",
    "    try: \n",
    "        to_clean = json.loads('{' + to_clean + '}')\n",
    "        #print(to_clean)\n",
    "        #add to df\n",
    "        \n",
    "        df_martha.at[index, 'Empathy'] = to_clean['Empathy']\n",
    "        df_martha.at[index, 'Traditional_Stalker'] = to_clean['Traditional_Stalker']\n",
    "    except: \n",
    "        fix_manually.append((index, to_clean))\n",
    "        print('Cannot clean')\n",
    "        print(to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fix a problem case manually\n",
    "#for row 312 put empathy to 0 and traditional stalker to 99\n",
    "df_martha.at[312, 'Empathy'] = 0\n",
    "df_martha.at[312, 'Traditional_Stalker'] = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_martha.to_csv('data/martha_empathy_labels.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
